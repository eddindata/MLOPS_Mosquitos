{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Estudios\\DMC\\Especializacion MLE\\env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
    "from keras.models import Model,Sequential\n",
    "from keras.optimizers import Adam,SGD,RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "\n",
    "from pathlib import Path\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = 'E:/Estudios/DMC/Especializacion MLE/Dataset/images'\n",
    "picture_size = 96\n",
    "batch_size  = 64\n",
    "var_seed = 17\n",
    "no_of_classes = 2\n",
    "epochs = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1124 images belonging to 2 classes.\n",
      "Found 280 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen_train = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255, validation_split=0.2)\n",
    "\n",
    "train_set = datagen_train.flow_from_directory(#dataPath+\"train\",\n",
    "                                            dataPath,\n",
    "                                            subset=\"training\",\n",
    "                                            target_size = (picture_size,picture_size),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'categorical',\n",
    "                                            shuffle = True, \n",
    "                                            seed  = var_seed)\n",
    "\n",
    "val_set = datagen_train.flow_from_directory(#dataPath+\"test\",\n",
    "                                            dataPath,\n",
    "                                            subset=\"validation\",\n",
    "                                            target_size = (picture_size,picture_size),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'categorical',\n",
    "                                            shuffle = False, \n",
    "                                            seed  = var_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Estudios\\DMC\\Especializacion MLE\\env\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\Estudios\\DMC\\Especializacion MLE\\env\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 3, 3, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               2359808   \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 512)               2048      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17077570 (65.15 MB)\n",
      "Trainable params: 2361858 (9.01 MB)\n",
      "Non-trainable params: 14715712 (56.14 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16 = tf.keras.applications.VGG16(input_shape=(picture_size, picture_size, 3), include_top=False, weights='imagenet')\n",
    "vgg16.trainable = False\n",
    "\n",
    "vgg16_model = tf.keras.Sequential([\n",
    "    vgg16,\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dense(512),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    tf.keras.layers.Dense(no_of_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = Adam(learning_rate= 0.0001)\n",
    "vgg16_model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"models/color/vgg16_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0.0001,\n",
    "                          patience=6,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_learningrate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=6,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "callbacks_list = [early_stopping,checkpoint,reduce_learningrate]\n",
    "\n",
    "vgg16_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 1: val_accuracy improved from -inf to 0.90357, saving model to models/color\\vgg16_model.h5\n",
      "18/18 [==============================] - 17s 904ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2130 - val_accuracy: 0.9036 - lr: 0.0010\n",
      "Epoch 2/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9929\n",
      "Epoch 2: val_accuracy did not improve from 0.90357\n",
      "18/18 [==============================] - 16s 873ms/step - loss: 0.0421 - accuracy: 0.9929 - val_loss: 0.5448 - val_accuracy: 0.8357 - lr: 0.0010\n",
      "Epoch 3/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9982\n",
      "Epoch 3: val_accuracy improved from 0.90357 to 0.92500, saving model to models/color\\vgg16_model.h5\n",
      "18/18 [==============================] - 16s 879ms/step - loss: 0.0022 - accuracy: 0.9982 - val_loss: 0.2039 - val_accuracy: 0.9250 - lr: 0.0010\n",
      "Epoch 4/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9982\n",
      "Epoch 4: val_accuracy did not improve from 0.92500\n",
      "18/18 [==============================] - 16s 877ms/step - loss: 0.0098 - accuracy: 0.9982 - val_loss: 0.2111 - val_accuracy: 0.9143 - lr: 0.0010\n",
      "Epoch 5/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9964\n",
      "Epoch 5: val_accuracy did not improve from 0.92500\n",
      "18/18 [==============================] - 16s 878ms/step - loss: 0.0143 - accuracy: 0.9964 - val_loss: 0.3323 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 6/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9991\n",
      "Epoch 6: val_accuracy did not improve from 0.92500\n",
      "18/18 [==============================] - 16s 870ms/step - loss: 0.0062 - accuracy: 0.9991 - val_loss: 0.3187 - val_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 7/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9956\n",
      "Epoch 7: val_accuracy did not improve from 0.92500\n",
      "18/18 [==============================] - 16s 877ms/step - loss: 0.0074 - accuracy: 0.9956 - val_loss: 0.2455 - val_accuracy: 0.9036 - lr: 0.0010\n",
      "Epoch 8/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 0.9973\n",
      "Epoch 8: val_accuracy improved from 0.92500 to 0.92857, saving model to models/color\\vgg16_model.h5\n",
      "18/18 [==============================] - 16s 878ms/step - loss: 0.0068 - accuracy: 0.9973 - val_loss: 0.2887 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 9/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9982Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.92857\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "18/18 [==============================] - 16s 869ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.2660 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Epoch 9: early stopping\n",
      "CPU times: total: 14min 34s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = vgg16_model.fit(train_set,\n",
    "                                epochs=epochs,\n",
    "                                validation_data = val_set,\n",
    "                                callbacks=callbacks_list\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 3, 3, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               4718848   \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28308098 (107.99 MB)\n",
      "Trainable params: 4719874 (18.00 MB)\n",
      "Non-trainable params: 23588224 (89.98 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet = tf.keras.applications.ResNet50(input_shape=(picture_size, picture_size, 3), include_top=False, weights='imagenet')\n",
    "resnet.trainable = False\n",
    "\n",
    "resnet_model = tf.keras.Sequential([\n",
    "    resnet,\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(256),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "\n",
    "    tf.keras.layers.Dense(no_of_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = Adam(learning_rate= 0.0001)\n",
    "resnet_model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"models/color/resnet_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0.0001,\n",
    "                          patience=6,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_learningrate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=6,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "callbacks_list = [early_stopping,checkpoint,reduce_learningrate]\n",
    "\n",
    "resnet_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.2922 - accuracy: 0.8728\n",
      "Epoch 1: val_accuracy improved from -inf to 0.57857, saving model to models/color\\resnet_model.h5\n",
      "18/18 [==============================] - 21s 1s/step - loss: 0.2922 - accuracy: 0.8728 - val_loss: 3.1860 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 2/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1036 - accuracy: 0.9644\n",
      "Epoch 2: val_accuracy did not improve from 0.57857\n",
      "18/18 [==============================] - 11s 626ms/step - loss: 0.1036 - accuracy: 0.9644 - val_loss: 3.2411 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 3/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9858\n",
      "Epoch 3: val_accuracy did not improve from 0.57857\n",
      "18/18 [==============================] - 11s 614ms/step - loss: 0.0571 - accuracy: 0.9858 - val_loss: 3.8143 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 4/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9858\n",
      "Epoch 4: val_accuracy did not improve from 0.57857\n",
      "18/18 [==============================] - 10s 558ms/step - loss: 0.0485 - accuracy: 0.9858 - val_loss: 4.3272 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 5/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9911\n",
      "Epoch 5: val_accuracy did not improve from 0.57857\n",
      "18/18 [==============================] - 10s 554ms/step - loss: 0.0360 - accuracy: 0.9911 - val_loss: 4.1861 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 6/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9947\n",
      "Epoch 6: val_accuracy did not improve from 0.57857\n",
      "18/18 [==============================] - 10s 557ms/step - loss: 0.0219 - accuracy: 0.9947 - val_loss: 3.7946 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 7/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9947Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.57857\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "18/18 [==============================] - 10s 550ms/step - loss: 0.0207 - accuracy: 0.9947 - val_loss: 4.5606 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 7: early stopping\n",
      "CPU times: total: 1min 49s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = resnet_model.fit(train_set,\n",
    "                                epochs=epochs,\n",
    "                                validation_data = val_set,\n",
    "                                callbacks=callbacks_list\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 96, 96, 64)        1792      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 96, 96, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 96, 96, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 48, 48, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 48, 48, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 48, 48, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 48, 48, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 24, 24, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 24, 24, 128)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 24, 24, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 24, 24, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 12, 12, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 12, 12, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 12, 12, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 6, 6, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 6, 6, 256)         1638656   \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 6, 6, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 3, 3, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 3, 3, 512)         2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 1, 1, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5705474 (21.76 MB)\n",
      "Trainable params: 5701762 (21.75 MB)\n",
      "Non-trainable params: 3712 (14.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "own_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64,(3,3),padding = 'same',input_shape = (96,96,3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),   \n",
    "    tf.keras.layers.Dropout(0.25),   \n",
    "    \n",
    "    \n",
    "    tf.keras.layers.Conv2D(128,(3,3),padding = 'same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),   \n",
    "    tf.keras.layers.Dropout(0.25),    \n",
    "\n",
    "    tf.keras.layers.Conv2D(128,(3,3),padding = 'same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),   \n",
    "    tf.keras.layers.Dropout(0.25),       \n",
    "\n",
    "    tf.keras.layers.Conv2D(256,(3,3),padding = 'same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),   \n",
    "    tf.keras.layers.Dropout(0.30),  \n",
    "    \n",
    "    tf.keras.layers.Conv2D(256,(5,5),padding = 'same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),   \n",
    "    tf.keras.layers.Dropout(0.30),     \n",
    "    \n",
    "    tf.keras.layers.Conv2D(512,(5,5),padding = 'same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),   \n",
    "    tf.keras.layers.Dropout(0.30), \n",
    "       \n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dense(512),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    tf.keras.layers.Dense(no_of_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = Adam(learning_rate= 0.0001)\n",
    "own_model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "own_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"models/color/own_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0.001,\n",
    "                          patience=6,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_learningrate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=6,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.001)\n",
    "\n",
    "callbacks_list = [early_stopping,checkpoint,reduce_learningrate]\n",
    "\n",
    "\n",
    "own_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5860 - accuracy: 0.7162\n",
      "Epoch 1: val_accuracy improved from -inf to 0.57857, saving model to models/color\\own_model.h5\n",
      "18/18 [==============================] - 18s 871ms/step - loss: 0.5860 - accuracy: 0.7162 - val_loss: 0.9449 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 2/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.8737\n",
      "Epoch 2: val_accuracy did not improve from 0.57857\n",
      "18/18 [==============================] - 15s 830ms/step - loss: 0.2857 - accuracy: 0.8737 - val_loss: 3.5470 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 3/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1795 - accuracy: 0.9262\n",
      "Epoch 3: val_accuracy did not improve from 0.57857\n",
      "18/18 [==============================] - 15s 844ms/step - loss: 0.1795 - accuracy: 0.9262 - val_loss: 3.2013 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 4/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1577 - accuracy: 0.9324\n",
      "Epoch 4: val_accuracy did not improve from 0.57857\n",
      "18/18 [==============================] - 15s 860ms/step - loss: 0.1577 - accuracy: 0.9324 - val_loss: 3.3702 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 5/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1067 - accuracy: 0.9617\n",
      "Epoch 5: val_accuracy did not improve from 0.57857\n",
      "18/18 [==============================] - 16s 863ms/step - loss: 0.1067 - accuracy: 0.9617 - val_loss: 2.7377 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 6/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9786\n",
      "Epoch 6: val_accuracy did not improve from 0.57857\n",
      "18/18 [==============================] - 15s 825ms/step - loss: 0.0562 - accuracy: 0.9786 - val_loss: 1.5238 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 7/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9929\n",
      "Epoch 7: val_accuracy improved from 0.57857 to 0.64286, saving model to models/color\\own_model.h5\n",
      "18/18 [==============================] - 15s 834ms/step - loss: 0.0235 - accuracy: 0.9929 - val_loss: 0.7918 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 8/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9875\n",
      "Epoch 8: val_accuracy did not improve from 0.64286\n",
      "18/18 [==============================] - 15s 810ms/step - loss: 0.0350 - accuracy: 0.9875 - val_loss: 1.8156 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 9/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9893\n",
      "Epoch 9: val_accuracy did not improve from 0.64286\n",
      "18/18 [==============================] - 15s 819ms/step - loss: 0.0291 - accuracy: 0.9893 - val_loss: 1.7856 - val_accuracy: 0.5857 - lr: 0.0010\n",
      "Epoch 10/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9956\n",
      "Epoch 10: val_accuracy did not improve from 0.64286\n",
      "18/18 [==============================] - 15s 826ms/step - loss: 0.0174 - accuracy: 0.9956 - val_loss: 0.8266 - val_accuracy: 0.6143 - lr: 0.0010\n",
      "Epoch 11/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9929\n",
      "Epoch 11: val_accuracy improved from 0.64286 to 0.80714, saving model to models/color\\own_model.h5\n",
      "18/18 [==============================] - 15s 826ms/step - loss: 0.0136 - accuracy: 0.9929 - val_loss: 0.3744 - val_accuracy: 0.8071 - lr: 0.0010\n",
      "Epoch 12/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9938\n",
      "Epoch 12: val_accuracy did not improve from 0.80714\n",
      "18/18 [==============================] - 14s 793ms/step - loss: 0.0196 - accuracy: 0.9938 - val_loss: 0.7042 - val_accuracy: 0.6357 - lr: 0.0010\n",
      "Epoch 13/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9982\n",
      "Epoch 13: val_accuracy did not improve from 0.80714\n",
      "18/18 [==============================] - 15s 804ms/step - loss: 0.0075 - accuracy: 0.9982 - val_loss: 0.8031 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 14/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9956\n",
      "Epoch 14: val_accuracy did not improve from 0.80714\n",
      "18/18 [==============================] - 15s 810ms/step - loss: 0.0119 - accuracy: 0.9956 - val_loss: 0.9939 - val_accuracy: 0.6357 - lr: 0.0010\n",
      "Epoch 15/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.9991\n",
      "Epoch 15: val_accuracy did not improve from 0.80714\n",
      "18/18 [==============================] - 15s 806ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.9788 - val_accuracy: 0.6464 - lr: 0.0010\n",
      "Epoch 16/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 16: val_accuracy did not improve from 0.80714\n",
      "18/18 [==============================] - 15s 805ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.5032 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 17/75\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9956Restoring model weights from the end of the best epoch: 11.\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.80714\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "18/18 [==============================] - 14s 800ms/step - loss: 0.0089 - accuracy: 0.9956 - val_loss: 0.5622 - val_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 17: early stopping\n",
      "CPU times: total: 22min 53s\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = own_model.fit(train_set,\n",
    "                                epochs=epochs,\n",
    "                                validation_data = val_set,\n",
    "                                callbacks=callbacks_list\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images belonging to 1 classes.\n",
      "1/1 [==============================] - 0s 217ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mosquito es Culex con un 100.0 % de probabilidad\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('E:/Estudios/DMC/Especializacion MLE/models/color/vgg16_model.h5')\n",
    "image_path = 'E:/Estudios/DMC/Especializacion MLE/Dataset/val'\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
    "picture_size = 96\n",
    "no_of_classes = 1\n",
    "\n",
    "image_val = datagen.flow_from_directory(    image_path, \n",
    "                                            target_size = (picture_size,picture_size),\n",
    "                                            class_mode = 'categorical',\n",
    "                                            batch_size = 1,\n",
    "                                            shuffle = False)\n",
    "\n",
    "for x_batch, y_batch in image_val:\n",
    "    processed_image = x_batch\n",
    "    break\n",
    "\n",
    "predictions = model.predict(image_val)\n",
    "\n",
    "# Mostrar la imagen procesada\n",
    "plt.imshow(processed_image[0])  # processed_image es un tensor, tomamos el primer elemento\n",
    "plt.axis('off')  # Desactiva los ejes\n",
    "plt.show()\n",
    "\n",
    "first_column = predictions[:, 0]\n",
    "\n",
    "if predictions[:, 0]>0.5:\n",
    "    print('El mosquito es Aedes', round(float(first_column[0]*100), 2), '% de probabilidad')\n",
    "else:\n",
    "    print('El mosquito es Culex con un', round(float((1-first_column[0])*100), 2), '% de probabilidad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
